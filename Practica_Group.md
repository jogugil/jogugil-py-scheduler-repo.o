# Lab: Building a Custom Kubernetes Scheduler in Python
##  Learning Objectives
 * By the end of this lab, you will:
    - Understand how the **scheduler** interacts with the **API Server**.
    - Implement a **custom scheduler** that:
    - Finds **Pending Pods** with a given `schedulerName`.
    - Chooses a **Node** according to a scheduling policy.
    - **Binds** the Pod to that Node through the Kubernetes API.
    - Compare **polling vs event-driven (watch)** models.
    - Deploy your scheduler into a **kind cluster** and observe its behavior.

  ---

# RealizaciÃ³n de la prÃ¡ctica:
 ## ðŸ§° Step 0 â€” Set up the environment
 
 We set up the environment using the required installation prerequisites. We then followed the steps described in 
 section `A` of the `README.md`.




 In the environment we prepared, we executed the first step:
 
 ## âš™ Step 1 â€” Observe the Default Scheduler-. 
 
 1. Identify the running scheduler
```Bash
kubectl -n kube-system get pods -l component=kube-scheduler
kubectl -n kube-system logs -l component=kube-scheduler
```
 2.  Schedule a simple pod:
```Bash
kubectl run test --image=nginx --restart=Never
kubectl get pods -o wide
```

### âœ…**Checkpoint 1:**
Describe the path:
    kubectl run â†’ Pod created â†’ Scheduler assigns Node â†’ kubelet starts Pod.
 
<p align="center">
<img src="https://github.com/jogugil/jogugil-py-scheduler-repo.o/blob/main/img/fugura1-1.png" width="850">
  <br>
  <em>Figure 2: Verification of the default scheduler and scheduling of a test Pod.</em>
</p>

âœ… **DescripciÃ³n del flujo de scheduling en Kubernetes**

La **Figura 2** muestra la ejecuciÃ³n de los comandos utilizados para verificar que el scheduler por defecto estÃ¡ en funcionamiento y para observar cÃ³mo se programa un Pod sencillo dentro del clÃºster creado con Kind. A partir de los resultados obtenidos, podemos describir el funcionamiento interno del sistema cuando programamos un Pod:

**a) Enviamos la orden de creaciÃ³n del Pod**  
Ejecutamos `kubectl run test --image=nginx --restart=Never`, lo que provoca que el cliente `kubectl` envÃ­e al API Server un objeto Pod para ser creado. En este momento, el Pod se registra pero aÃºn no tiene un nodo asignado.

**b) El Pod queda inicialmente en estado *Pending***  
Tras su creaciÃ³n, el API Server almacena el Pod con `status=Pending`, ya que todavÃ­a no ha sido asociado a ningÃºn nodo del clÃºster.

**c) El scheduler detecta el nuevo Pod sin asignar**  
El `kube-scheduler`, que aparece ejecutÃ¡ndose como se muestra en la Figura 2, observa periÃ³dicamente los Pods pendientes mediante sus mecanismos internos de *informers*.  
Detecta que el Pod reciÃ©n creado no tiene un nodo asociado (`.spec.nodeName` vacÃ­o).

**d) El scheduler selecciona un nodo adecuado**  
Una vez detectado el Pod pendiente, el scheduler evalÃºa los nodos disponibles.  
En nuestro entorno Kind, la asignaciÃ³n habitual es al nodo de control (`sched-lab-control-plane`).  
El scheduler realiza entonces el *binding* del Pod, actualizando su campo `.spec.nodeName`.

**e) El kubelet del nodo asignado inicia el contenedor**  
Tras el binding, el kubelet del nodo seleccionado recibe la nueva especificaciÃ³n, descarga la imagen `nginx` si no estÃ¡ disponible y comienza a crear el contenedor.  
El estado del Pod pasa a `ContainerCreating` y finalmente a `Running`.

En conjunto, estos pasos confirman que el flujo interno es el esperado:

**kubectl run â†’ API Server crea el Pod â†’ Scheduler asigna nodo â†’ Kubelet ejecuta el contenedor**,  
tal como se observa en la secuencia mostrada en **Figura 2**.



 ## ðŸ§± Step 2 â€” Project Setup

 Initialize Project
 
 ```Bash
mkdir py-scheduler && cd py-scheduler
python -m venv .venv && source .venv/bin/activate
pip install kubernetes==29.0.0
touch scheduler.py
 ```
Directory Structure

 ```Bash
py-scheduler/
â”œâ”€â”€ scheduler.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ rbac-deploy.yaml
â”œâ”€â”€ test-pod.yaml
â””â”€â”€ requirements.txt
 ```
## ðŸ§  Step 3 â€” Implement the Polling Scheduler
### âœ…**Checkpoint 2:**

***Understand the control loop:***
    - **Observe**: *list unscheduled Pods:*    
    - **Decide**: *pick a Node*       
    - **Act**: *bind the Pod*
 
## ðŸ³ðŸ”ðŸ§ª Step 4 5 y 6 â€” Build and Deploy. RBAC & Deployment. Test Your Scheduler


### âœ…**Checkpoint 3:**

***Your scheduler should log a message like:***
    - Bound default/test-pod -> kind-control-plane

## ðŸ§© Step 7 â€” Event-Driven Scheduler (Watch API)

### âœ…**Checkpoint 4:**
***Compare responsiveness and efficiency between polling and watch approaches.***

## ðŸ§© Step 8 â€” Policy Extensions

1. Label-based node filtering
```Bash
nodes = [n for n in api.list_node().items
if "env" in (n.metadata.labels or {}) and
n.metadata.labels["env"] == "prod"]
```
2. Taints and tolerations Use `node.spec.taints` and `pod.spec.tolerations` to filter nodes
before scoring.
3. Backoff / Retry Use exponential backoff when binding fails due to transient API errors.
4. Spread policy Distribute similar Pods evenly across Nodes.

### âœ… **Checkpoint 5:**
***Demonstrate your extended policy via pod logs and placement.***


# ðŸ§ Reflection Discussion
- ***Why is it important that your scheduler writes a Binding object instead of patching a Pod directly?***
- ***What are the trade-offs between polling vs event-driven models?***
- ***How do taints and tolerations interact with your scheduling logic?***
- ***What are real-world policies you could implement using this framework?***
