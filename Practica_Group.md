# Lab: Building a Custom Kubernetes Scheduler in Python
##  Learning Objectives
 * By the end of this lab, you will:
    - Understand how the **scheduler** interacts with the **API Server**.
    - Implement a **custom scheduler** that:
    - Finds **Pending Pods** with a given `schedulerName`.
    - Chooses a **Node** according to a scheduling policy.
    - **Binds** the Pod to that Node through the Kubernetes API.
    - Compare **polling vs event-driven (watch)** models.
    - Deploy your scheduler into a **kind cluster** and observe its behavior.

  ---

# Realizaci√≥n de la pr√°ctica:
 ## üß∞ Step 0 ‚Äî Set up the environment
 
 We set up the environment using the required installation prerequisites. We then followed the steps described in 
 section `A` of the `README.md`.

 In the environment we prepared, we executed the first step:
 
 ## ‚öô Step 1 ‚Äî Observe the Default Scheduler-. 
 
 1. Identify the running scheduler
```Bash
kubectl -n kube-system get pods -l component=kube-scheduler
kubectl -n kube-system logs -l component=kube-scheduler
```
 2.  Schedule a simple pod:
```Bash
kubectl run test --image=nginx --restart=Never
kubectl get pods -o wide
```

### ‚úÖ**Checkpoint 1:**
Describe the path:
    kubectl run ‚Üí Pod created ‚Üí Scheduler assigns Node ‚Üí kubelet starts Pod.
 
<p align="center">
<img src="https://github.com/jogugil/jogugil-py-scheduler-repo.o/blob/main/img/fugura1-1.png" width="850">
  <br>
  <em>Figure 1: Verification of the default scheduler and scheduling of a test Pod.</em>
</p>

‚úÖ **Descripci√≥n del flujo de scheduling en Kubernetes**

La **Figura 1** muestra la ejecuci√≥n de los comandos utilizados para verificar que el scheduler por defecto est√° en funcionamiento y para observar c√≥mo se programa un Pod sencillo dentro del cl√∫ster creado con Kind. A partir de los resultados obtenidos, podemos describir el funcionamiento interno del sistema cuando programamos un Pod:

**a) Enviamos la orden de creaci√≥n del Pod**  
Ejecutamos `kubectl run test --image=nginx --restart=Never`, lo que provoca que el cliente `kubectl` env√≠e al API Server un objeto Pod para ser creado. En este momento, el Pod se registra pero a√∫n no tiene un nodo asignado.

**b) El Pod queda inicialmente en estado *Pending***  
Tras su creaci√≥n, el API Server almacena el Pod con `status=Pending`, ya que todav√≠a no ha sido asociado a ning√∫n nodo del cl√∫ster.

**c) El scheduler detecta el nuevo Pod sin asignar**  
El `kube-scheduler`, que aparece ejecut√°ndose como se muestra en la Figura 1, observa peri√≥dicamente los Pods pendientes mediante sus mecanismos internos de *informers*.  
Detecta que el Pod reci√©n creado no tiene un nodo asociado (`.spec.nodeName` vac√≠o).

**d) El scheduler selecciona un nodo adecuado**  
Una vez detectado el Pod pendiente, el scheduler eval√∫a los nodos disponibles.  
En nuestro entorno Kind, la asignaci√≥n habitual es al nodo de control (`sched-lab-control-plane`).  
El scheduler realiza entonces el *binding* del Pod, actualizando su campo `.spec.nodeName`.

**e) El kubelet del nodo asignado inicia el contenedor**  
Tras el binding, el kubelet del nodo seleccionado recibe la nueva especificaci√≥n, descarga la imagen `nginx` si no est√° disponible y comienza a crear el contenedor.  
El estado del Pod pasa a `ContainerCreating` y finalmente a `Running`.

En conjunto, estos pasos confirman que el flujo interno es el esperado:

**kubectl run ‚Üí API Server crea el Pod ‚Üí Scheduler asigna nodo ‚Üí Kubelet ejecuta el contenedor**,  
tal como se observa en la secuencia mostrada en **Figura 2**.



 ## üß± Step 2 ‚Äî Project Setup

 Initialize Project
 
 ```Bash
mkdir py-scheduler && cd py-scheduler
python -m venv .venv && source .venv/bin/activate
pip install kubernetes==29.0.0
touch scheduler.py
 ```
Directory Structure

 ```Bash
py-scheduler/
‚îú‚îÄ‚îÄ scheduler.py
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ rbac-deploy.yaml
‚îú‚îÄ‚îÄ test-pod.yaml
‚îî‚îÄ‚îÄ requirements.txt
 ```

<p align="center">
<img src="https://github.com/jogugil/jogugil-py-scheduler-repo.o/blob/main/img/Figura2.png" width="850">
  <br>
  <em>Figure 2: We create the local directory containing the files required to deploy the polling scheduler.</em>

</p>

## üß† Step 3 ‚Äî Implement the Polling Scheduler
### ‚úÖ**Checkpoint 2:**

***Understand the control loop:***
    - **Observe**: *list unscheduled Pods:*    
    - **Decide**: *pick a Node*       
    - **Act**: *bind the Pod*

Para implementar el scheduler basado en *polling*, se ha seguido el patr√≥n cl√°sico de los controladores de Kubernetes: **Observar ‚Üí Decidir ‚Üí Actuar**. El c√≥digo proporcionado ([variants/polling/scheduler.py](https://github.com/jogugil/jogugil-py-scheduler-repo.o/blob/main/variants/polling/scheduler.py)
) implementa este ciclo mediante consultas peri√≥dicas al API Server. A continuaci√≥n describimos cada fase y su relaci√≥n directa con el c√≥digo del scheduler.

---

‚úÖ **1. Observar: listar los Pods no programados**

En el bucle principal, el scheduler consulta peri√≥dicamente al API Server para obtener los Pods que cumplen:

- **no tienen nodo asignado**, es decir, est√°n en estado `Pending` (`spec.nodeName=`), y  
- **solicitan expl√≠citamente el scheduler personalizado** (`spec.schedulerName == args.scheduler_name`) (Debe ser `my_scheduler`).

```python
pods = api.list_pod_for_all_namespaces(
    field_selector="spec.nodeName="
).items

for pod in pods:
    if pod.spec.scheduler_name != args.scheduler_name:
        continue
```
  As√≠, s√≥lo cogemos los Pods pendientes de asignaci√≥n, es decir, que a√∫n no tienen un nodo asignado (spec.nodeName vac√≠o), lo que normalmente corresponde a Pods en estado Pending.
  
 ‚úÖ 2. Decidir: seleccionar un nodo
La l√≥gica de decisi√≥n est√° en:
 ```python
node = choose_node(api, pod)
 ```
La funci√≥n `choose_node()` realiza lo siguiente:

a) Obtiene la lista completa de nodos: `nodes = api.list_node().items`  
b) Cuenta cu√°ntos Pods est√°n ya asignados a cada nodo: `cnt = sum(1 for p in pods if p.spec.node_name == n.metadata.name)`  
c) Selecciona el nodo con menos Pods, aplicando as√≠ una estrategia sencilla de ‚Äúmenor carga‚Äù: `if cnt < min_cnt:`  
 
   ‚úÖ 3. Actuar: realizar el binding del Pod√ß
    ```python
   bind_pod(api, pod, node_name)
    ```
El binding consiste en:
    a) crear una referencia al nodo: `target = client.V1ObjectReference(kind="Node", name=node_name)`
    b) crear la estructura V1Binding: `body = client.V1Binding(target=target, metadata=client.V1ObjectMeta(name=pod.metadata.name))`
    c) enviarla al API Server para completar la asignaci√≥n: `api.create_namespaced_binding(pod.metadata.namespace, body)`

Este paso actualiza el campo .spec.nodeName del Pod.  Y a partir de aqu√≠, el kubelet del nodo asignado detecta la nueva asignaci√≥n y comienza la creaci√≥n del contenedor correspondiente.
    
## üê≥üîêüß™ Step 4, 5 y 6 ‚Äî Build and Deploy. RBAC & Deployment. Test Your Scheduler (polling)

Los pasos que hemos realizado para probar el `scheduler_polling` personalizado dentro del cl√∫ster son:

**a) Build:** Construimos nuestra imagen Docker etiquet√°ndola como `latest` a partir del directorio actual. Esta imagen servir√° como base para nuestras ejecuciones.

```Bash
docker build -t my-py-scheduler:latest .
```

**b) Load Image:** Cargamos esa imagen en el cl√∫ster Kind llamado `sched-lab` para poder usarla en nuestros despliegues.

```Bash
kind load docker-image my-py-scheduler:latest --name sched-lab
```

**c) RBAC:** Aplicamos las reglas RBAC que autorizan a nuestro scheduler a autenticarse y operar contra el API Server con los permisos definidos (roles del scheduler). Adem√°s, desplegamos nuestro scheduler (`my_scheduler`) en el cluster (`Control Plane`).

```Bash
kubectl apply -f rbac-deploy.yaml
```

d) API Server: Consultamos al API Server para obtener el listado de Pods con la etiqueta app=my-scheduler y verificar que nuestro scheduler se ha desplegado correctamente.

```Bash
kubectl -n kube-system get pods -l app=my-scheduler
```

Nota: Hemos **encontrado un error** al realizar los pasos `c` y `d`. Esto nos pasa porque en el manifiesto `rbac-deploy.yaml` no contiene una pol√≠tica de pull. Entonces Kubernetes aplica su pol√≠tica por defecto (`imagePullPolicy = Always`). Y marca el Pod como `ImagePullBackOff`. Por tanto, a√±adimos una pol√≠tica de Pull en el manifiesto (`imagePullPolicy: Never`):

```yaml
containers:
- name: scheduler
  image: my-py-scheduler:latest
  imagePullPolicy: Never
  args: ["--scheduler-name","my-scheduler"]
```

Podenmos poner:

- `imagePullPolicy: Never`: Para desarrollo local con Kind; asegura que Kubernetes solo usa la imagen local.

- `imagePullPolicy: IfNotPresent`: Si la imagen est√° en el nodo, la usa. Si no, intenta descargarla. Adecuado para despliegues mixtos.
  
- `imagePullPolicy: Always`: Fuerza siempre el pull y hace fallar im√°genes locales.

¬øPor qu√© ponemos `Never`?

Utilizamos esa pol√≠tica ya que estamos trabajando con una imagen local creada a mano y cargada con:

```Bash
kind load docker-image my-py-scheduler:latest --name sched-lab
```
Esto hace que la imagen est√© disponible solo dentro de los nodos del cl√∫ster Kind, pero NO existe en Docker Hub ni en ning√∫n registry remoto.

Por tanto:

Si Kubernetes intenta descargarla ‚Üí fallar√° (ImagePullBackOff)

Si Kubernetes usa la imagen local ‚Üí funcionar√°

Y para obligar a Kubernetes a usar la imagen local del nodo, la pol√≠tica exacta es:

  Volvemos a ejecutar el deployment:

```Bash
  kubectl -n kube-system delete pod -l app=my-scheduler
  kubectl apply -f rbac-deploy.yaml
  kubectl -n kube-system get pods -l app=my-scheduler
```

**e) Test Pod:** Creamos un Pod de prueba y lo desplegamos en el cl√∫ster para comprobar que nuestro scheduler (`my_scheduler`) obtiene el Pod creado y le asigna un nodo.

```Bash
kubectl apply -f test-pod.yaml
kubectl get pods -o wide
kubectl -n kube-system logs deploy/my-scheduler
```

Noa II: Una vez modificado el manifiesto y lanzado el scheduler con el nuevo, nos encontramos con un nuevo error:

        jogugil@PHOSKI:~/kubernetes_ejemplos/scheduler/py-scheduler-repo.o/py-scheduler$ kubectl -n kube-system logs -f my-scheduler-6fbbc9c795-7h7gz [polling] scheduler starting‚Ä¶ name=my-scheduler error: Invalid value for `target`, must not be `None`

Hemops encontrado que el API se ha modificado y debemos cambiar el codigo python:

https://stackoverflow.com/questions/50729834/kubernetes-python-client-api-create-namespaced-binding-method-shows-target-nam?utm_source=chatgpt.com


En el nuevo API : https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#binding-v1-core

La forma correcta de asignar un Pod es mediante:

PATCH /api/v1/namespaces/{namespace}/pods/{name}
spec.nodeName = <node>


que en Python es exactamente:


```python
api.patch_namespaced_pod(
    name=pod.metadata.name,
    namespace=pod.metadata.namespace,
    body={"spec": {"nodeName": node_name}}
)
```

por tanto, la nueva funci√≥n quedar√°:

```python
def bind_pod(api, pod, node_name):
    patch = {"spec": {"nodeName": node_name}}
    api.patch_namespaced_pod(
        name=pod.metadata.name,
        namespace=pod.metadata.namespace,
        body=patch
    )
```

**Modificamos** el c√≥digo del **scheduler polling** para generar el binding del Pod y asignarle un nodo de ejecuci√≥n usando la nueva versi√≥n del API (patch directo del `nodeName` en lugar de `create_namespaced_binding`). Tras aplicar la modificaci√≥n, borramos los Pods existentes y la imagen cargada, para poder reconstruirla y ejecutar todo nuevamente desde cero.

1. Borrar la imagen local:
```Bash
docker rmi my-py-scheduler:latest
```

3. Borrar la imagen dentro del nodo Kind:
```Bash
docker exec -it sched-lab-control-plane crictl rmi my-py-scheduler:latest
```

4. (Opcional) Borrar la imagen en nodos worker si existieran:
```Bash
docker exec -it sched-lab-worker crictl rmi my-py-scheduler:latest
```

5. Construirla de nuevo:
```Bash
docker build -t my-py-scheduler:latest .
```

6. Cargarla otra vez en Kind:
```Bash
kind load docker-image my-py-scheduler:latest --name sched-lab
```

7. Borramos test_pod:
```Bash
kubectl delete pod test-pod
```

9. Hacemos el deploy nuevamente del scheduler modificado:
```Bash
kubectl apply -f rbac-deploy.yaml
```

10. Hacemos el deploy de Test_pod:
```Bash
kubectl apply -f test-pod.yaml
```

11. Comprobamos los logs que no hayan nuevos errores:
```Bash
kubectl -n kube-system logs -f deploy/my-scheduler
```

**Nota III: Nuevo error: Al ejecutar el scheduler modificado sobre test_POD**

Modificamos el c√≥digo Python del scheduler y ya vemos que se est√° ejecutando. Sin embargo, aparece un nuevo error porque no tenemos permisos para trabajar con Pods en el namespace `default`. Por tanto, tendremos que ajustar el manifiesto del Pod o los permisos del ServiceAccount.

```bash
# Comprobamos que nuestro scheduler est√° corriendo
kubectl -n kube-system get pods -l app=my-scheduler
NAME                            READY   STATUS    RESTARTS   AGE
my-scheduler-6fbbc9c795-4drdb   1/1     Running   0          4m36s

# Revisamos los logs del scheduler
kubectl -n kube-system logs -f deploy/my-scheduler
[polling] scheduler starting‚Ä¶ name=my-scheduler
[TRACE] bind_pod called for kube-system/test-pod -> sched-lab-control-plane

# Creamos el Pod de prueba
kubectl apply -f test-pod.yaml
pod/test-pod created

# Volvemos a revisar los logs del scheduler
kubectl -n kube-system logs -f deploy/my-scheduler
[TRACE] bind_pod called for default/test-pod -> sched-lab-control-plane
Traceback (most recent call last):
  File "/app/scheduler.py", line 27, in bind_pod
    api.patch_namespaced_pod(
kubernetes.client.exceptions.ApiException: (403)
Reason: Forbidden
HTTP response body: {"message":"pods \"test-pod\" is forbidden: User \"system:serviceaccount:kube-system:my-scheduler\" cannot patch resource \"pods\" in API group \"\" in the namespace \"default\""}
```
El nuevo error se debe a que el ServiceAccount `my-scheduler` no tiene permisos RBAC para modificar Pods en el namespace `default`. Para solucionarlo, modificamos el manifiesto de `test-pod.yaml` para que se cree en `kube-system`, donde s√≠ tenemos permisos, de la siguiente manera:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: kube-system
spec:
  schedulerName: my-scheduler
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.9
```

**Otra alternativa m√°s elegante ser√≠a modificar el manifiesto RBAC para que nuestro scheduler (`my-scheduler`) tenga permisos tambi√©n sobre los Pods creados en el namespace `default`.** Esto permite mantener los Pods en `default` y que nuestro scheduler personalizado pueda asignar nodos sin necesidad de cambiar el namespace de los Pods.  

**Contras:**  
- Dar permisos al scheduler sobre `default` expone un riesgo de seguridad: cualquier Pod en `default` podr√≠a ser manipulado por `my-scheduler`.  
- Hay que asegurarse de no sobreescribir roles cr√≠ticos ni dar m√°s permisos de los estrictamente necesarios.  

**Manifiesto RBAC modificado para permitir acceso a Pods en `default`:**  

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-scheduler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-scheduler-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch", "patch", "update"]
# Esta secci√≥n es la que a√±adimos o modificamos para dar permisos a nuestro scheduler
# sobre los Pods en cualquier namespace (incluido 'default')
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-scheduler-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-scheduler-role
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-scheduler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels: {app: my-scheduler}
  template:
    metadata:
      labels: {app: my-scheduler}
    spec:
      serviceAccountName: my-scheduler
      containers:
      - name: scheduler
        image: my-py-scheduler:latest
        imagePullPolicy: Never
        args: ["--scheduler-name","my-scheduler"]
# Aqu√≠ no necesitamos cambiar nada; el scheduler seguir√° usando el ServiceAccount con los permisos ampliados

Una vez modificado el manifiesto del Pod, ejecutamos `los pasos del 1 al 11` y comprobamos que el scheduler personalizado (`my_scheduler`) se ejecuta correctamente y asigna un nodo al Pod creado, sin generar errores de permisos.


**f) M√©tricas:** Para comrpbar la latencia y la carga generada por `my_scheduler`, en su versi√≥n `polling`, lanzamos estos comandos:

f-1) Comprobar latencia

```Bash
kubectl -n kube-system logs -l app=my-scheduler-polling --timestamps
```

- Tiempo t0: momento en que ejecutamos `kubectl run`

- Tiempo t1: primera l√≠nea del log donde el scheduler muestra que ha detectado un Pod pendiente (aparece "Detected Pending Pod").

- Latencia --> Œît = t1 ‚Äì t0

f-2) Comprobar carga

1.Medir las peticiones generadas hacia el API Server:

```Bash
kubectl -n kube-system logs -l kube-apiserver | grep LIST | wc -l
```

2.Medir consumo del Pod del scheduler:
```Bash
kubectl top pod -n kube-system | grep my-scheduler
```

f-3) Medir eficiencia del flujo del scheduling con un √∫nico Pod

Aunque solo haya un Pod, puede medir c√≥mo cambia la ‚Äúpipeline de scheduling‚Äù entre polling y watch.
 
 
- Lanzamos un Pod sencillo:
```Bash
kubectl run test --image=nginx --restart=Never
```

- Obtenemos eventos:
```Bash
kubectl get events --sort-by=.metadata.creationTimestamp
```

- Revisamos:

* N√∫mero de logs redundantes del scheduler polling: Cu√°ntas veces el scheduler polling imprime ‚ÄúNo pending pods‚Äù o ‚ÄúChecking pending pods‚Äù.

```Bash
kubectl -n kube-system logs -l app=my-scheduler-polling | grep "Checking pending pods" | wc -l
```
* Cambios de estado Pending ‚Üí Running:
```Bash
kubectl get pod test -o jsonpath='{.status.phase}'
```

- Antes: Pending

- Despu√©s: Running

- Tiempo total = Scheduling + Container start.

* Cu√°ntas veces el scheduler polling detecta el Pod:

(para polling)
```Bash
kubectl -n kube-system logs -l app=my-scheduler-polling | grep "Detected Pending Pod" | wc -l
```

(para watch)

```Bash
kubectl -n kube-system logs -l app=my-scheduler-watch | grep "Pod added" | wc -l
```
  
### ‚úÖ**Checkpoint 3:**

***Your scheduler should log a message like:***
    - Bound default/test-pod -> kind-control-plane
    

## üß© Step 7 ‚Äî Event-Driven Scheduler (Watch API)

En este paso modificamos `my_scheduler`del cluster para que se ejecute la versi√≥n `watch`. Realizamos todos los pasos anteriores para cargar la imagen con mi nuevo `scheduler_watch` y calcular las nuevas m√©tricas.

Notar que para obtener las m√©tricas de cada uno de los shceulers persoinalizamos hemos creado lso siguientes scripts.

- ***`metrics-polling.sh`***
```Bash
#!/bin/bash

SCHED_NS="kube-system"
SCHED_LABEL="app=my-scheduler-polling"
TEST_POD="test-metric-polling"

echo "======================================================="
echo " M√âTRICAS DEL SCHEDULER POLLING"
echo "======================================================="

echo "[1] Lanzamos Pod de prueba"
T0=$(date +%s%3N)
kubectl run $TEST_POD --image=nginx --restart=Never >/dev/null 2>&1

echo "[2] Esperamos a que se generen logs"
sleep 2

echo "[3] Obtenemos logs del scheduler polling"
kubectl -n $SCHED_NS logs -l $SCHED_LABEL --timestamps > polling.log

echo "[4] Calculamos latencia (t1 - t0)"
TS_LINE=$(grep -m1 "Detected Pending Pod" polling.log | awk '{print $1}')

if [[ -z "$TS_LINE" ]]; then
    echo "No se encontr√≥ 'Detected Pending Pod' en los logs del scheduler."
else
    # Convertir timestamp ISO8601 a epoch ms
    T1=$(date -d "$TS_LINE" +%s%3N)
    LATENCY=$((T1 - T0))
    echo "t0 (inicio): $T0 ms"
    echo "t1 (detecci√≥n): $T1 ms"
    echo "Latencia total: $LATENCY ms"
fi

echo
echo "[5] N√∫mero de peticiones LIST al API Server"
LISTS=$(kubectl -n kube-system logs -l component=kube-apiserver | grep LIST | wc -l)
echo "Peticiones LIST: $LISTS"

echo
echo "[6] Consumo de CPU del scheduler polling"
kubectl top pod -n $SCHED_NS | grep my-scheduler-polling || echo "top no disponible"

echo
echo "[7] Eventos Kubernetes (Pending ‚Üí Running)"
kubectl get events --sort-by=.metadata.creationTimestamp > events.log
grep $TEST_POD events.log

echo
echo "[8] Logs redundantes del polling"
REDUNDANT=$(grep -c "Checking pending pods" polling.log)
echo "Iteraciones del bucle polling: $REDUNDANT"

echo
echo "[9] N√∫mero de detecciones del Pod"
DETECTIONS=$(grep -c "Detected Pending Pod" polling.log)
echo "Detecciones totales: $DETECTIONS"

echo
echo "[10] Estado final del Pod"
STATE=$(kubectl get pod $TEST_POD -o jsonpath='{.status.phase}')
echo "Estado: $STATE"

echo
echo "[11] Limpieza del Pod de prueba"
kubectl delete pod $TEST_POD >/dev/null 2>&1
echo "Limpieza completada"

echo "======================================================="
echo " FIN DEL SCRIPT POLLING"
echo "======================================================="
```
- ***`metrics-watch.sh`***

```Bash
#!/bin/bash

SCHED_NS="kube-system"
SCHED_LABEL="app=my-scheduler-watch"
TEST_POD="test-metric-watch"

echo "======================================================="
echo " M√âTRICAS DEL SCHEDULER WATCH"
echo "======================================================="

echo "[1] Lanzamos Pod de prueba"
T0=$(date +%s%3N)
kubectl run $TEST_POD --image=nginx --restart=Never >/dev/null 2>&1

echo "[2] Esperamos a que se generen eventos"
sleep 1

echo "[3] Obtenemos logs del scheduler watch"
kubectl -n $SCHED_NS logs -l $SCHED_LABEL --timestamps > watch.log

echo "[4] Calculamos latencia (primer evento)"
TS_LINE=$(grep -m1 "Pod added" watch.log | awk '{print $1}')

if [[ -z "$TS_LINE" ]]; then
    echo "No se encontr√≥ 'Pod added' en los logs del scheduler watch."
else
    T1=$(date -d "$TS_LINE" +%s%3N)
    LATENCY=$((T1 - T0))
    echo "t0 (inicio): $T0 ms"
    echo "t1 (evento): $T1 ms"
    echo "Latencia total: $LATENCY ms"
fi

echo
echo "[5] N√∫mero de eventos Watch"
ADDED=$(grep -c "Pod added" watch.log)
UPDATED=$(grep -c "Pod updated" watch.log)
echo "Eventos 'Pod added': $ADDED"
echo "Eventos 'Pod updated': $UPDATED"

echo
echo "[6] Peticiones LIST al API Server"
LISTS=$(kubectl -n kube-system logs -l component=kube-apiserver | grep LIST | wc -l)
echo "N√∫mero de LIST: $LISTS"

echo
echo "[7] Consumo de CPU del scheduler watch"
kubectl top pod -n $SCHED_NS | grep my-scheduler-watch || echo "top no disponible"

echo
echo "[8] Eventos Kubernetes (Pending ‚Üí Running)"
kubectl get events --sort-by=.metadata.creationTimestamp | grep $TEST_POD

echo
echo "[9] Estado final del Pod"
STATE=$(kubectl get pod $TEST_POD -o jsonpath='{.status.phase}')
echo "Estado: $STATE"

echo
echo "[10] Limpieza del Pod de prueba"
kubectl delete pod $TEST_POD >/dev/null 2>&1
echo "Limpieza completada"

echo "======================================================="
echo " FIN DEL SCRIPT WATCH"
echo "======================================================="
```

  


### ‚úÖ**Checkpoint 4:**
***Compare responsiveness and efficiency between polling and watch approaches.***

## üß© Step 8 ‚Äî Policy Extensions

1. Label-based node filtering
```Bash
nodes = [n for n in api.list_node().items
if "env" in (n.metadata.labels or {}) and
n.metadata.labels["env"] == "prod"]
```
2. Taints and tolerations Use `node.spec.taints` and `pod.spec.tolerations` to filter nodes
before scoring.
3. Backoff / Retry Use exponential backoff when binding fails due to transient API errors.
4. Spread policy Distribute similar Pods evenly across Nodes.

### ‚úÖ **Checkpoint 5:**
***Demonstrate your extended policy via pod logs and placement.***


# üß†Reflection Discussion


- ***Why is it important that your scheduler writes a Binding object instead of patching a Pod directly?***
  
> ### Importancia de usar un `Binding` en lugar de modificar directamente un Pod
>
> Porque el uso de un `Binding` es el mecanismo definido por Kubernetes para asignar un Pod a un nodo.  
> En un principio, este mecanismo permite escalabilidad y fiabilidad del cl√∫ster, ya que comprueba si los nodos tienen permisos para ejecutar dicho Pod y si la carga del nodo permite ejecutarlo. Esto permite mantener un balance de carga y garantizar la seguridad en la ejecuci√≥n de los contenedores.  
> La asignaci√≥n se realiza de forma **at√≥mica y segura**, es decir, o se asigna o no se asigna, evitando condiciones de carrera.  
> 
> Adem√°s, todo se realiza a trav√©s del **API Server**, lo que garantiza que el flujo de control sea el correcto dentro del sistema. Esto permite tambi√©n mantener un sistema **auditable**, √∫til para depuraci√≥n y trazabilidad.

- ***What are the trade-offs between polling vs event-driven models?***

> ### Ventajas y desventajas del modelo de polling
>
> **Ventajas:**
> - Muy f√°cil de implementar.  
> - No necesita controladores sofisticados.  
> - Tolera fallos temporales de conexi√≥n.
>
> **Desventajas:**
> - Introduce **latencia**: un Pod puede tardar en ser detectado.  
> - Genera **carga innecesaria** en el API Server por las consultas repetidas.  
> - No escala bien en cl√∫steres grandes.
 
- ***How do taints and tolerations interact with your scheduling logic?***

> - Un **taint** en un nodo sirve para ‚Äúrepeler‚Äù Pods que no lo toleren.  
> - Una **toleration** en un Pod indica que puede ejecutarse en un nodo con ese taint.
>
> Para un scheduler personalizado:
> - a) Debemos **filtrar nodos cuyo taint no pueda ser tolerado** por el Pod.  
> - b) Si ignoramos esto, podr√≠amos bindear un Pod a un nodo donde **nunca podr√° ejecutarse**, quedando permanentemente en `Pending`.  
>
> **Ejemplo:**  
> Un Pod sin tolerations **no debe** ser programado en un nodo con el taint `NoSchedule`.
>
> Por tanto, un scheduler completo debe:
> - Leer los taints del nodo.  
> - Leer las tolerations del Pod.  
> - Excluir nodos incompatibles antes de tomar una decisi√≥n.
 
- ***What are real-world policies you could implement using this framework?***

> El framework permite implementar pol√≠ticas reales como:
>
> - a) ***[Nodo menos cargado](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)*** (la que usamos).
>
> - b) ***[Resource Bin Packing](https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing/)***: llenar nodos al m√°ximo antes de usar nuevos.
>
> - c) ***[Affinity y Anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)***  
>     - Separar cargas sensibles.  
>     - Agrupar Pods que trabajan juntos.
>
> - d) ***Ahorro energ√©tico***  
>     - Consolidar cargas para apagar nodos poco usados.  
>     - Elegir nodos m√°s eficientes.
>
> - e) ***Topolog√≠a y rendimiento***  
>     - Elegir nodos seg√∫n regi√≥n, zona, latencia, GPU‚Ä¶  
>     - [Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)  
>     - [Node Labels](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#built-in-node-labels)  
>     - [Topology Manager / NUMA](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-manager/)
>
> - f) ***Prioridades y SLAs***  
>     - Colocar Pods prioritarios en nodos espec√≠ficos.  
>     - [Pod Priority & Preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)  
>     - [QoS Classes](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/)

